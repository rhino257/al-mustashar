import traceback
import logging
import time # Added for timing
import os # Added for checking TESTING_MODE
from typing import List, Optional, Dict, Any, AsyncGenerator
from datetime import datetime, timezone

from langchain_core.documents import Document as LangchainDocument
from langchain_core.runnables import RunnableConfig # Added import

from retrieval_graph.state import AgentState
from retrieval_graph.configuration import AgentConfiguration # Added import
from prompts.synthesis_prompts import (
    SYNTHESIS_PROMPT_ALMUSTASHAR_ARABIC,
    SYNTHESIS_PROMPT_ALDHAKI_ARABIC,
    SYNTHESIS_PROMPT_ALGHABI_ARABIC
)
from prompts.conversational_prompts import CONVERSATIONAL_PROMPT_ARABIC # Added import
from shared.utils import load_chat_model # Added import

logger = logging.getLogger(__name__)

# Helper to adapt Langchain Document to the expected structure for context building
class SourceDocumentAdapter:
    def __init__(self, doc: LangchainDocument):
        self.text_content: Optional[str] = doc.page_content
        self.metadata: Dict[str, Any] = doc.metadata or {}

def _build_synthesis_context(
    state: AgentState,
    final_documents: List[SourceDocumentAdapter]
) -> str:
    """Builds the context string for the synthesis prompt, including history."""
    # Extract user_query from the last human message in AgentState.messages
    user_query = ""
    if state.messages and isinstance(state.messages[-1].content, str):
        user_query = state.messages[-1].content # Assumes last message is HumanMessage with query

    # preliminary_answer: For now, we'll assume it's not available in AgentState.
    # The prompt has logic to handle its absence.
    preliminary_answer = None # state.preliminary_answer if hasattr(state, 'preliminary_answer') else None

    chat_history_messages = []
    if state.messages and len(state.messages) > 1: # Exclude the current user query
        for msg in state.messages[:-1]:
            role = "User" if msg.type == "human" else "Assistant"
            if isinstance(msg.content, str):
                 chat_history_messages.append({"role": role, "content": msg.content})

    history_str = "\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in chat_history_messages])
    if history_str:
         synthesis_context_str = f"Conversation History:\n{history_str}\n\n"
    else:
         synthesis_context_str = ""

    synthesis_context_str += f"Current User Query: \"{user_query}\"\n\n"

    if preliminary_answer and len(final_documents) != 1:
        synthesis_context_str += f"Preliminary Answer (Generated by LLM - Verify with Context): \"{preliminary_answer}\"\n\n"

    synthesis_context_str += "Relevant Law Articles Context (Retrieved and Ranked - PRIMARY SOURCE OF TRUTH):\n"
    if not final_documents:
        synthesis_context_str += "[No relevant articles found]\n"
    else:
        for doc_adapter in final_documents:
            if doc_adapter.text_content:
                metadata = doc_adapter.metadata
                law_name = metadata.get('law_name', 'Unknown Law')
                art_num = metadata.get('article_number', 'N/A')
                synthesis_context_str += f"\n--- Source: {law_name} - Article {art_num} ---\n{doc_adapter.text_content}\n"
    return synthesis_context_str

async def synthesize_yemeni_legal_answer_node(state: AgentState, *, config: RunnableConfig) -> Dict[str, Any]: # Added config to signature
    """
    Node to synthesize the final ARABIC legal answer based on retrieved documents and chat history.
    """
    node_name = "synthesize_yemeni_legal_answer_node"
    logger.info(f"--- Executing {node_name} ---")
    logger.info(f"[Timestamp: {datetime.now(timezone.utc).isoformat()}] [BE Log Marker] STEP_START: {node_name}")
    start_time = time.perf_counter()

    updated_state: Dict[str, Any] = {
        "final_answer": None,
        "llm_output_stream": None,
        "error": None,
        "error_step": None,
    }

    try:
        app_config = AgentConfiguration.from_runnable_config(config)
        llm = load_chat_model(app_config.response_model, app_config) # Pass app_config

        if not llm:
            logger.error(f"[{node_name}] LLM (for response_model) not available or failed to load.")
            updated_state["error"] = "LLM (for response_model) not configured or failed to load."
            updated_state["error_step"] = node_name
            updated_state["final_answer"] = "حدث خطأ في إعدادات النظام."
        else:
            final_langchain_docs: List[LangchainDocument] = []
            if state.final_documents_for_synthesis:
                final_langchain_docs = state.final_documents_for_synthesis
                logger.info(f"[{node_name}] Using {len(final_langchain_docs)} documents from AgentState.final_documents_for_synthesis for synthesis.")
            else:
                logger.info(f"[{node_name}] No documents available in AgentState.final_documents_for_synthesis for synthesis.")
                final_langchain_docs = []

            final_docs_adapted = [SourceDocumentAdapter(doc) for doc in final_langchain_docs]
            synthesis_context_str = _build_synthesis_context(state, final_docs_adapted)
            
            agent_persona = app_config.agent_persona # This is a string like "المستشار", "الذكي", "الغبي"
            selected_prompt_template_str = ""

            if agent_persona == "الذكي":
                selected_prompt_template_str = SYNTHESIS_PROMPT_ALDHAKI_ARABIC
                logger.info(f"[{node_name}] Using الذكي persona prompt.")
            elif agent_persona == "الغبي":
                selected_prompt_template_str = SYNTHESIS_PROMPT_ALGHABI_ARABIC
                logger.info(f"[{node_name}] Using الغبي persona prompt.")
            elif agent_persona == "المستشار":
                selected_prompt_template_str = SYNTHESIS_PROMPT_ALMUSTASHAR_ARABIC
                logger.info(f"[{node_name}] Using المستشار persona prompt.")
            else:
                # Default to Almustashar if persona is unrecognized or not set
                selected_prompt_template_str = SYNTHESIS_PROMPT_ALMUSTASHAR_ARABIC
                logger.warning(f"[{node_name}] Unrecognized or missing agent_persona ('{agent_persona}'). Defaulting to المستشار persona prompt.")
            
            prompt = selected_prompt_template_str.format(synthesis_context_str=synthesis_context_str)

            try: # Inner try for LLM call
                logger.info(f"[Timestamp: {datetime.now(timezone.utc).isoformat()}] [BE Log Marker] LLM_CALL_WITH_CONTEXT: Context Length: {len(synthesis_context_str)} characters, Num Docs: {len(final_docs_adapted)}")
                logger.info(f"[Timestamp: {datetime.now(timezone.utc).isoformat()}] [BE Log Marker] LLM_STREAM_START: Providing astream generator.")
                
                logger.info(f"[{node_name}] Collecting full text for internal state.")
                full_text_chunks = []
                # ProductionGeminiChatModel expects messages in OpenAI dict format
                # The 'prompt' here is a string, so we need to wrap it.
                # For synthesis, the prompt is complex and acts as the system/user instruction.
                synthesis_messages = [{"role": "user", "content": prompt}]
                
                # Using the new streaming method from ProductionGeminiChatModel
                # Assuming default strategy "fallback" and "normal" priority for synthesis
                
                # Prepare user metadata for Portkey
                portkey_user_metadata_synthesis = None
                user_id_synthesis = getattr(state, "user_id", None)
                if user_id_synthesis: # type: ignore
                    portkey_user_metadata_synthesis = {
                        "user_id": user_id_synthesis, 
                        "_user": user_id_synthesis 
                    }
                    user_name_synthesis = getattr(state, "user_name", None)
                    if user_name_synthesis:
                        portkey_user_metadata_synthesis["user_name"] = user_name_synthesis
                    phone_number_synthesis = getattr(state, "phone_number", None)
                    if phone_number_synthesis:
                        portkey_user_metadata_synthesis["phone_number"] = phone_number_synthesis
                    logger.info(f"[{node_name}] Passing user_metadata to Portkey for synthesis (internal collection): {portkey_user_metadata_synthesis}")
                else:
                    logger.info(f"[{node_name}] No user_id in state, not passing specific user metadata for synthesis.")

                logger.info(f"[{node_name}] Creating stream for llm_output_stream.")
                # This single stream will be consumed by the router for SSE.
                # The router will also accumulate the full text for saving.
                llm_stream_for_output = llm.astream_chat_completion_content(
                    messages=synthesis_messages,
                    temperature=0.1, # Lowered temperature for more consistent synthesis
                    strategy="fallback", # Or choose a specific strategy
                    priority="normal",
                    user_metadata=portkey_user_metadata_synthesis
                    # model_override="gemini-2.0-flash" # Rely on Portkey Config ID to set the model
                )
                updated_state["llm_output_stream"] = llm_stream_for_output
                # final_answer will be populated by the router from the stream.
                # Set to None here as it's not collected by this node anymore.
                updated_state["final_answer"] = None
                logger.info(f"[{node_name}] Stream assigned to llm_output_stream. final_answer set to None (will be accumulated by router).")

                # Always clear the stream from the state dict that will be checkpointed. (Comment remains for context, but no stream is in state dict)
                # The actual stream is yielded by the node for SSE, but not persisted in the state record.
                # This dictionary is what updates the AgentState.
                # logger.info(f"[{node_name}] Clearing llm_output_stream from returned state dictionary for checkpointing compatibility.")
                # updated_state["llm_output_stream"] = None


            except Exception as e: # Inner except for LLM call
                logger.error(f"[Timestamp: {datetime.now(timezone.utc).isoformat()}] [BE Log Marker] LLM_SYNTHESIS_ERROR: Details: {traceback.format_exc()}")
                logger.error(f"[{node_name}] Error during synthesis LLM call: {e}")
                updated_state["error"] = f"Error in {node_name} LLM call: {str(e)}"
                updated_state["error_step"] = node_name
                updated_state["final_answer"] = "حدث خطأ فني أثناء صياغة الإجابة النهائية."
    
    except Exception as outer_e: # Outer except for setup errors
        logger.error(f"[{node_name}] Outer error (e.g. config loading): {outer_e}", exc_info=True)
        updated_state["error"] = f"Outer error in {node_name}: {str(outer_e)}"
        updated_state["error_step"] = node_name
        updated_state["final_answer"] = "حدث خطأ عام في إعدادات النظام."
        # Ensure llm_output_stream is None if error occurs before it's set
        if "llm_output_stream" not in updated_state:
            updated_state["llm_output_stream"] = None
            
    finally:
        end_time = time.perf_counter()
        logger.info(f"--- {node_name} execution time: {end_time - start_time:.4f} seconds ---")
        logger.info(f"[Timestamp: {datetime.now(timezone.utc).isoformat()}] [BE Log Marker] STEP_END: {node_name}")
    
    return updated_state


def prepare_synthesis_node(state: AgentState) -> Dict[str, Any]:
    """
    Prepares the final list of documents for the synthesis node.
    It checks for documents from both retrieval and direct lookup paths.
    """
    node_name = "prepare_synthesis_node"
    logger.info(f"--- Executing {node_name} ---")
    
    # Use getattr for safe access on TypedDict
    retrieved_docs = getattr(state, "retrieved_documents", [])
    direct_lookup_docs = getattr(state, "direct_lookup_results", [])
    
    final_docs = []
    if retrieved_docs:
        logger.info(f"[{node_name}] Found {len(retrieved_docs)} documents from retrieval.")
        final_docs.extend(retrieved_docs)
    if direct_lookup_docs:
        logger.info(f"[{node_name}] Found {len(direct_lookup_docs)} documents from direct lookup.")
        final_docs.extend(direct_lookup_docs)
        
    logger.info(f"[{node_name}] Total documents for synthesis: {len(final_docs)}")
    
    return {"final_documents_for_synthesis": final_docs}


# --- Conversational Response Node ---

def _build_conversational_llm_prompt(state: AgentState) -> str:
    """Builds the prompt string for conversational responses."""
    user_query = ""
    if state.messages and isinstance(state.messages[-1].content, str):
        user_query = state.messages[-1].content

    chat_history_messages = []
    if state.messages and len(state.messages) > 1: # Exclude current user query
        for msg in state.messages[:-1]:
            role = "User" if msg.type == "human" else "Assistant"
            if isinstance(msg.content, str):
                chat_history_messages.append({"role": role, "content": msg.content})
    
    history_str = "\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in chat_history_messages])
    history_section = f"Conversation History:\n{history_str}\n\n" if history_str else ""

    return CONVERSATIONAL_PROMPT_ARABIC.format(history_section=history_section, user_query=user_query)

async def handle_conversational_query_node(state: AgentState, *, config: RunnableConfig) -> Dict[str, Any]: # Added config to signature
    """
    Node to generate a conversational ARABIC response using chat history.
    Bypasses RAG retrieval and synthesis.
    """
    node_name = "handle_conversational_query_node"
    logger.info(f"--- Executing {node_name} ---")
    logger.info(f"[Timestamp: {datetime.now(timezone.utc).isoformat()}] [BE Log Marker] STEP_START: {node_name}")
    start_time = time.perf_counter()

    updated_state: Dict[str, Any] = {
        "conversational_response": None,
        "llm_output_stream": None,
        "error": None,
        "error_step": None,
    }

    try:
        app_config = AgentConfiguration.from_runnable_config(config)
        llm = load_chat_model(app_config.response_model, app_config) # Pass app_config

        if not llm:
            logger.error(f"[{node_name}] LLM (for response_model) not available or failed to load for conversational node.")
            updated_state["error"] = "LLM (for response_model) not configured or failed to load for conversational node."
            updated_state["error_step"] = node_name
            updated_state["conversational_response"] = "حدث خطأ في إعدادات النظام للردود الحوارية."
        else:
            user_query = ""
            if state.messages and isinstance(state.messages[-1].content, str):
                user_query = state.messages[-1].content
            
            if not user_query:
                updated_state["error"] = "User query is missing for conversational response."
                updated_state["error_step"] = node_name
                logger.error(f"[{node_name}] Error: {updated_state['error']}")
                updated_state["conversational_response"] = "آسف، لم أفهم ما قلته. هل يمكنك إعادة الصياغة؟"
            else:
                prompt = _build_conversational_llm_prompt(state)
                try: # Inner try for LLM call
                    logger.info(f"[{node_name}] Attempting LLM call for conversational response.")
                    logger.info(f"[Timestamp: {datetime.now(timezone.utc).isoformat()}] [BE Log Marker] LLM_STREAM_START (Conversational): Providing astream generator.")
                    
                    logger.info(f"[{node_name}] Collecting full text for internal state.")
                    full_text_chunks = []
                    # The 'prompt' for conversational is also a single string.
                    # We need to format it as OpenAI messages.
                    # The prompt already includes history and user query.
                    conversational_messages = [{"role": "user", "content": prompt}]

                    # Prepare user metadata for Portkey
                    portkey_user_metadata_conv = None
                    user_id_conv = getattr(state, "user_id", None)
                    if user_id_conv: # type: ignore
                        portkey_user_metadata_conv = {
                            "user_id": user_id_conv, 
                            "_user": user_id_conv
                        }
                        user_name_conv = getattr(state, "user_name", None)
                        if user_name_conv:
                            portkey_user_metadata_conv["user_name"] = user_name_conv
                        phone_number_conv = getattr(state, "phone_number", None)
                        if phone_number_conv:
                            portkey_user_metadata_conv["phone_number"] = phone_number_conv
                        logger.info(f"[{node_name}] Passing user_metadata to Portkey for conversational (internal collection): {portkey_user_metadata_conv}")
                    else:
                        logger.info(f"[{node_name}] No user_id in state, not passing specific user metadata for conversational.")
                    
                    logger.info(f"[{node_name}] Creating stream for llm_output_stream.")
                    # This single stream will be consumed by the router for SSE.
                    # The router will also accumulate the full text for saving.
                    llm_stream_for_output = llm.astream_chat_completion_content(
                        messages=conversational_messages,
                        temperature=0.5,
                        strategy="fallback", # Or choose a specific strategy
                        priority="normal",
                        user_metadata=portkey_user_metadata_conv
                        # model_override="gemini-2.0-flash" # Rely on Portkey Config ID to set the model
                    )
                    updated_state["llm_output_stream"] = llm_stream_for_output
                    # conversational_response will be populated by the router from the stream.
                    # Set to None here as it's not collected by this node anymore.
                    updated_state["conversational_response"] = None
                    logger.info(f"[{node_name}] Stream assigned to llm_output_stream. conversational_response set to None (will be accumulated by router).")

                    # Always clear the stream from the state dict that will be checkpointed. (Comment remains for context)
                    # logger.info(f"[{node_name}] Clearing llm_output_stream from returned state dictionary for checkpointing compatibility.")
                    # updated_state["llm_output_stream"] = None # This was already commented out, keeping for context.
                    
                except Exception as e: # Inner except for LLM call
                    logger.error(f"[{node_name}] Error during conversational LLM call: {e}", exc_info=True)
                    updated_state["error"] = f"LLM error in {node_name}: {str(e)}"
                    updated_state["error_step"] = node_name
                    updated_state["conversational_response"] = "آسف، حدث خطأ فني."
                    updated_state["llm_output_stream"] = None
    
    except Exception as outer_e: # Outer except for setup errors
        logger.error(f"[{node_name}] Outer error (e.g. config loading): {outer_e}", exc_info=True)
        updated_state["error"] = f"Outer error in {node_name}: {str(outer_e)}"
        updated_state["error_step"] = node_name
        updated_state["conversational_response"] = "حدث خطأ عام في إعدادات النظام."
        if "llm_output_stream" not in updated_state: # Should already be None from initialization
            updated_state["llm_output_stream"] = None

    finally:
        end_time = time.perf_counter()
        logger.info(f"--- {node_name} execution time: {end_time - start_time:.4f} seconds ---")
        logger.info(f"[Timestamp: {datetime.now(timezone.utc).isoformat()}] [BE Log Marker] STEP_END: {node_name}")
    
    return updated_state
