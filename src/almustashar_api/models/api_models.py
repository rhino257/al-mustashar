from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any

# Model for the new GET request query parameters
class RagQueryRequest(BaseModel):
    """
    Request model for querying the RAG agent via GET parameters.
    """
    query: str = Field(..., max_length=4096, description="The user's input query.")
    chat_id: str = Field(..., max_length=128, description="The ID of the chat session.")
    pipeline_name: Optional[str] = Field("default", max_length=128, description="Name of the RAG pipeline/agent configuration to use.")
    ai_message_id: Optional[str] = Field(None, max_length=128, description="Client-provided AI message ID for resuming/tracking.")
    use_reranker: bool = Field(False, description="Whether to use a reranker.")

# Model for source documents
class SourceDocument(BaseModel):
    """
    Represents a source document retrieved by the RAG process.
    """
    id: Optional[str] = Field(None, description="Unique identifier for the source document.")
    content: str = Field(..., description="The content of the source document.")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional metadata for the source document (e.g., source_url, page).")

# Models for SSE (Server-Sent Events) data payloads
class SSEMetadata(BaseModel):
    """
    Metadata event sent at the beginning of an SSE stream.
    """
    chat_id: str = Field(..., description="The chat_id from the request.")
    ai_message_id: str = Field(..., description="A client-provided or newly generated unique ID for this AI interaction cycle.")
    file_processing_errors: List[Any] = Field([], description="Array of objects, if applicable. Can be empty.")

class SSEStreamInitiated(BaseModel):
    """
    Event signaling the initiation of the message stream.
    """
    message_id: str = Field(..., description="Corresponds to ai_message_id or a sub-id for this specific stream.")
    status: str = Field("processing", description="Status of the stream (e.g., 'processing', 'thinking').")
    isFinal: bool = Field(False, description="Always false for this event.")

class SSEMessageUpdate(BaseModel):
    """
    Event sent for each chunk of the LLM response.
    """
    message_id: str = Field(..., description="The message_id for the current stream.")
    delta: str = Field(..., description="The new chunk of text from the LLM.")
    cumulative_text: str = Field(..., description="The full accumulated text of the answer so far.")
    status: str = Field("streaming", description="Status of the message ('streaming').")
    isFinal: bool = Field(False, description="Always false for this event.")

class ChatIdInfo(BaseModel):
    """
    Information about the chat ID.
    """
    chat_id: str = Field(..., description="The chat_id from the request.")
    is_new_chat: bool = Field(False, description="If chat_id management implies new chat detection, update this. Otherwise, false is fine.")

class ErrorDetails(BaseModel):
    """
    Details for an error occurring during the streaming process.
    """
    error: str = Field(..., description="Short error type/name.")
    details: str = Field(..., description="Potentially longer traceback or more info.")
    user_facing_message: str = Field(..., description="Message to show the user.")

class SSEMessageFinalized(BaseModel):
    """
    The final event, signaling completion or error of the stream.
    """
    message_id: str = Field(..., description="The message_id for the current stream.")
    persistent_ai_message_id: str = Field(..., description="A unique ID for this AI response, generated by the backend.")
    full_content: Optional[str] = Field(None, description="The complete, final answer (if successful).") # Ensure this can be None
    status: str = Field(..., description="Status of the stream ('complete' or 'error').")
    isFinal: bool = Field(True, description="Always true for this event.")
    chat_id_info: ChatIdInfo = Field(..., description="Information about the chat ID.")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Optional metadata, including sources for RAG.")
    error_details: Optional[ErrorDetails] = Field(None, description="Only if status is 'error'.")


# Model for the new POST /chat-query endpoint
class NewAgentChatRequest(BaseModel):
    """
    Request model for the new agent chat query endpoint.
    """
    query: str = Field(..., max_length=4096, description="The user's input query.")
    chat_id: Optional[str] = Field(None, max_length=128, description="The ID of the chat session. If None, a new one might be created.")
    agent_persona: Optional[str] = Field("almustashar", max_length=128, description="The desired agent persona (e.g., 'almustashar', 'alzaki'). Defaults to 'almustashar'.")
    # Add any other fields that might be needed to initialize AgentState, e.g., files, specific config overrides
    # For example:
    # files: Optional[List[Dict[str, Any]]] = Field(default_factory=list, description="List of file metadata if any files are uploaded.")
    # use_reranker: bool = Field(False, description="Whether to use a reranker for this specific query.")
